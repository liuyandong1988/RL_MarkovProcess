# 强化学习三大求解思想：动态规划，蒙特卡洛，时序差分


## 1.基于Bellman方程的预测与控制（动态规划思想）

### 1.1 迭代策略评估
已知MDP中的所有信息，给定一个策略。使用IPE，去评价给定策略的好坏。并改进策略
状态空间S：S1 - S14非终止状态，ST终止状态；
行为空间A：{n, e, s, w} 对于任何非终止状态可以有东南西北移动四个行为；
转移概率P：任何试图离开方格世界的动作其位置将不会发生改变，其余条件下将100%地转移到动作指向的状态；
即时奖励R：任何在非终止状态间的转移得到的即时奖励均为-1，进入终止状态即时奖励为0；
衰减系数γ：1；
当前策略π：Agent采用随机行动策略，在任何一个非终止状态下有均等的几率采取任一移动方向这个行为，即π(n|•) = π(e|•) = π(s|•) = π(w|•) = 1/4

### 1.2 策略迭代
在当前策略上迭代计算v值，再根据v值贪婪地更新策略，如此反复多次，最终得到最优策略  和最优状态价值函数 。

### 1.3 价值迭代
定理：一个策略能够使得状态s获得最优价值，当且仅当：对于从状态s可以到达的任何状态s’，该策略能够使得状态s’的价值是最优价值：
问题：寻找最优策略π
解决方案：从初始状态价值开始同步迭代计算，最终收敛，整个过程中没有遵循任何策略。
注意：与策略迭代不同，在值迭代过程中，算法不会给出明确的策略，迭代过程其间得到的价值函数，不对应任何策略。

## 2. Monte Carlo
（1）我们把智能体放到环境的任意状态；
（2）从这个状态开始按照策略进行选择动作，并进入新的状态。
（3）重复步骤（2），直到最终状态；
（4）我们从最终状态开始向前回溯：计算每个状态的G值。
（5）重复1-4多次，然后平均每个状态的G值，这就是我们需要求的V值。
在实际引用中，蒙地卡罗虽然比动态规划消耗要少一点；而且并不需要知道整个环境模型。但蒙地卡罗有一个比较大的缺点，就是每一次游戏，都需要先从头走到尾，再进行回溯更新。
随机策略梯度就使用了Monte Carlo思想。

## 3. Temporal Difference 
TD算法对蒙地卡罗(MC)进行了改进：
（1） 和蒙地卡罗(MC)不同，TD算法只需要走N步。就可以开始回溯更新。
（2） 和蒙地卡罗(MC)一样：N步行为，每经过一个状态，把奖励记录下来。然后开始回溯。
（3）其实和蒙地卡罗一样，我们就假设N步之后，就到达了最终状态了。  假设“最终状态”上我们之前没有走过，所以这个状态上的纸是空白的。这个时候我们就当这个状态为0. - 假设“最终状态”上我们已经走过了，这个状态的V值，就是当前值。然后我们开始回溯。
Q-learning，Actor-Critic，DDPG都使用的TD思想。