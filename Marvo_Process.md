强化学习
1. 什么是马尔可夫性？
    下一状态只与当前**状态**有关，与以前的状态无关。

2. 什么是马尔可夫过程？
    无记忆的随机过程，由状态集合和**状态转移**表示。无记忆指马尔可夫性。随机指，状态依概率转移。

3. 什么是马尔可夫奖励过程（MRP）？
   在马尔科夫过程的基础上增加了**奖励R**和**衰减系数γ**。一个完整的马尔可夫奖励过程，有一个带有衰减系数的累积奖励值，叫做**回报**或者**收益**。一个确定状态下的回报期望，叫做**状态价值函数**。强化学习中一种解题思路就是求解状态价值函数。

4.	什么是马尔可夫决策过程（MDP）？
	相较于马尔科夫奖励过程，马尔科夫决定过程多了一个**行为**集合。与MRP中的状态价值函数对应，MDP过程有**行为状态价值函数**，对当前状态s执行某一具体行为a所能的到的收获的期望。把这个连续过程中决定行为的函数，叫做**策略函数**。策略是概率，对过程中的某一状态s采取可能的行为a的概率。

5. 状态价值函数与行为状态函数的关系
	确定状态下的状态价值函数等于，根据确定策略执行行为的概率乘以执行行为后的行为状态函数。确定状态下，执行确定行为的行为状态函数等于，立即奖励加上执行行为转移到新状态下的状态价值函数。

6. 贝尔曼方程与价值函数，最优策略
	强化学习的目的是找到最优策略，途径是最大化状态价值函数或者行为状态函数（优化值函数的角度，还有策略梯度和Actor-Critic两种思路）。贝尔曼方程是MRP，MDP对应状态价值函数和行为状态函数的数学描述。通过求解贝尔曼方程得到最优策略，三大类求解方法动态规划（Dynamic Programming）蒙特卡洛（Monte Carlo）时序差分（Temporal-difference）。
	
总结，我们用马尔可夫过程来描述强化学习。个体处于一个状态state，通过策略policy做出一个动作action，与环境environment交互，转移到另一状态next state，用一个奖励reward来评价状态state下执行动作action的好坏。马尔可夫是强化学习的理论基础，但在强化学习遇到Deep Neural Network后，马尔可夫越来越少的被提及，人们将状态扔进网络，训练出策略，得到动作。关注的重点放到了网络设计，模型训练。好的结果成为了最重要的评价指标。但回头看看去掉深度的强化学习能找到其理论体系的整体性，逻辑性。


